# see https://docs.mcp-agent.com/reference/configuration
$schema: ../../../schema/mcp-agent.config.schema.json

name: deep_research_agent
execution_engine: asyncio

logger:
  transports: [file]
  level: debug # debug, info, warning, error
  progress_display: true
  path_settings:
    # Logs rel
    path_pattern: "../logs/deep-research-agent-{unique_id}.jsonl"
    unique_id: "timestamp" # Options: "timestamp" or "session_id"
    timestamp_format: "%Y%m%d_%H%M%S"

mcp:
  # In this config file, all supported debugging flags or tools are used, 
  # like @modelcontextprotocol/inspector. See
  #   https://modelcontextprotocol.io/docs/tools/inspector#pypi-package
  #   https://pypi.org/project/mcp-server-fetch/ (example)
  # Note that @modelcontextprotocol/inspector is actually intended for interactive use.
  servers:
    fetch:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/inspector", "uvx", "mcp-server-fetch"]
    pubmed-central:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://www.ncbi.nlm.nih.gov/pmc/tools/api/", "--debug"]
    pubmed-gpt:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://api.ncbi.nlm.nih.gov/lit/", "--debug"]
    nih-clinical-trials:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://clinicaltrials.gov/api/", "--debug"]
    healthcare-repository:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://api.healthcarerepo.org/mcp/v1", "--debug"]
    medical-qa:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://api.medicalqa.ai/mcp/v1", "--debug"]
    bio-mcp:
      command: "npx"
      args: ["-y", "mcp-remote@latest", "https://biomcp.genomoncology.com/api/v1", "--debug"]
    filesystem:
      command: "npx"
      args: ["-y", "@modelcontextprotocol/server-filesystem", ".."]
      
# Since Ollama and OpenAI use the same internal code path, i.e.,
# mcp-agent's `OpenAIAugmentedLLM`, the definition for "openai:"
# here is actually for inference through ollama.
# Change the default model if you prefer to use another one.
openai:
  default_model: "gpt-4o-mini"
  reasoning_effort: "medium"
  base_url: "https://api.openai.com/v1"
  
anthropic:
  default_model: "claude-3-5-sonnet-20241022"
